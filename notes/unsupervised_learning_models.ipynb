{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cb0689",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "Focus: discovering patterns, structure, or representations from **unlabeled** data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27695df2",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Distance Metrics\n",
    "\n",
    "### Curse of Dimensionality \n",
    "\n",
    "### Manifold Hypothesis\n",
    "\n",
    "** make sure to include hypothesis about each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f942ee",
   "metadata": {},
   "source": [
    "## 1. Clustering\n",
    "**Goal**: Assign data to groups based on similarity.\n",
    "\n",
    "\n",
    "### (a) Classical Methods\n",
    "\n",
    "#### (i)  K-Means\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74501cf",
   "metadata": {},
   "source": [
    "#### (ii)  DBSCAN\n",
    "\n",
    "Limitations of DBSCAN: A fixed neighbourhood radius of $\\epsilon$ can be a poor choice if different clusters have different densities.\n",
    "\n",
    "Observations: The border points are not deterministically assigned to a cluser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b609be",
   "metadata": {},
   "source": [
    "#### (iii)  Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) \n",
    "\n",
    "HDBSCAN attempts to address some of the limitations of DBSCAN. \n",
    "\n",
    "##### Notation and Key Definitions\n",
    "First we describe the algorithm and the necessary notation. The algorithm will take as input a set of points $X=x_1, \\dots, x_n \\in \\mathbb{R}^d$ that we wish to cluster as well as two parameters $s=$ minPts, $m=$ minClusterSize, where the latter is the minimum number of points in any valid cluster the algorithm can output. We define $\\text{core}_s(x)$ to be the distance to the sth nearest neighbour to $x$. In order words  $\\text{core}_s(x)$ is the minimum value of $\\epsilon$ for which $x$ would be considered a core point in DBSCAN. Then we define the mutual reachability distance between $x$ and $y$ as\n",
    "\n",
    "$$\n",
    "\\text{mreach}(x,y) = \\max(\\text{core}_s(x), \\text{core}_s(y), d(x, y)).\n",
    "$$\n",
    "If clusters have different densities or there is noise then the standard distance computation can be misleading. For example if $x$ is in a dense region and $y$ is in a sparse region but $d(x, y)$ is small. We may not wish to cluster them together, and this is what the mutual reachability distance is correcting for. \n",
    "\n",
    "##### Mutual Reachability Graph and Dendogram:\n",
    "Once we have computed these values we can build a mutual reachability graph $G=(V, E)$ where $V=X$ and there is an edge between each $x, y \\in X$ with weight $w(x, y) =\\text{mreach}(x, y)$, from which we compute a minimum spanning tree $T$, which we will use to build a dendogram $D$ that dictates a cluster hierarchy as follows. Let $e_1, \\dots, e_{n-1}$ be the edges of $T$ sorted so that $w(e_1) \\geq \\dots \\geq w(e_{n-1})$.\n",
    "\n",
    "Create a root node of $D$ that signifies the cluster containing all of $X$. Suppose that edges $e_1, \\dots, e_i$ have all been processed, and consider the set of clusters $\\mathcal{C}$ are the set of clusters obtained by removing $e_1, \\dots, e_i, e_{i+1}, \\dots, e_j$ from $T$, where $j$ is the largest index such that $w(e_j)=w(e_{i+1})$.\n",
    "It should note that we only include edges $(x, y)$ in the graph such that $y$ is one of the $k$ closest neighbours of $x$.\n",
    "\n",
    "Then for each leaf $C$ of our dendogram $D$ we add as children all elements $C' \\in \\mathcal{C}$ satisfying $C' \\subsetneq C$ and $|C'| \\geq m$, as we wish for $D$ to contain possible choices of clusters.\n",
    "\n",
    "##### Stability Computation\n",
    "\n",
    "Now that we have built $D$ we need to select our clusters, but since we have not fixed a density value in advance HDBSCAN attempts to make more local decisions about choices of density using something called stability, which gives us a numeric value which we can use to rank clusters. \n",
    "\n",
    "We let $\\lambda(C)=\\frac{1}{d}$ where $d$ was the weight of the edges removed from $T$ when the cluster appeared in $\\mathcal{C}$. We set $\\lambda(X)=0$.\n",
    " Then for $x \\in C$ we define $\\lambda_{x}(C)=\\lambda(C')$, where $C'$ is the child of $C$ containing $x$. Here we defined the stability of all clusters, not just the valid ones.\n",
    "\n",
    " Then the stability of $C$ is \n",
    "\n",
    " $$\n",
    "\\text{stability}(C) = \\sum_{x \\in C}(\\lambda_x(C)-\\lambda(C))\n",
    " $$\n",
    "the intution being that once, clusters with high stability persist longer in the tree, and are therefore better candidates.\n",
    "<!-- here we can add intuition about the choice of the inverse relationship -->\n",
    "\n",
    "\n",
    "##### Cluster Selection\n",
    "<!-- Beginning with the solution containing the root cluster we replace a cluster $C$ with its children $C_1, \\dots, C_t$ if $\\sum_{j=1}^{t}\\text{stability}(C_j) > \\text{stability}(C)$. -->\n",
    "HDBSCAN employs a cluster selection selection algorithm, here we discuss the default option called, Excess of Mass(EOM), where the excess-of-mass of a given cluster $C$ is denoted $\\text{EOM}(C)$ and is computed with the following recursive formula, where $C_1$ and $C_2$ are the children of $C$ (if $C$ is not a leaf):\n",
    "\n",
    "$$ \\text{EOM}(C) = \n",
    "\\begin{cases}\n",
    "    0 & C \\text{ is a leaf}\\\\\n",
    "    \\text{EOM}(C_1) + \\text{EOM}(C_2) & |C| < m\\\\\n",
    "    \\max(\\text{stability}(C), \\text{EOM}(C_1) + \\text{EOM}(C_2)) & \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and if $\\text{EOM}(C) = \\text{stability}(C)$, then we say that $C$ is locally selected by the EOM algorithm. The output is then the set of locally selected clusters that have no ancestor in $D$ that is also selected by EOM satisfying $|C| \\geq m$, and does not contain the root.\n",
    "\n",
    "##### Summary\n",
    "\n",
    "1. Input: $X=x_1, \\dots, x_n \\in \\mathbb{R}^d$, m=minPts, s=minClusterSize.\n",
    "2. Build a mutual reachability graph $G$\n",
    "3. Let $T$ be a minimum spanning tree of $G$.\n",
    "4. Let $e_1, \\dots, e_{n-1}$ be the edges of $T$ sorted so that $w(e_1) \\geq \\dots \\geq w(e_{n-1})$.\n",
    "5. Build a dendrogram $D$.\n",
    "6. Construct solution using the  cluster selection method.\n",
    "\n",
    "##### Implementation\n",
    "[Here](../src/unsupervised_learning/HDBSCAN.py) is an implementation that allows for custom distance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284aa68",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### (b) Neural Methods \n",
    "  <!-- - DeepCluster (Caron et al.)\n",
    "  - SCAN\n",
    "  - DEC (Deep Embedded Clustering) -->\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Dimensionality Reduction\n",
    "**Goal**: Map high-dimensional data to a lower-dimensional space preserving structure.\n",
    "### (a) Linear\n",
    "#### PCA\n",
    "\n",
    "### (b) Non-Linear\n",
    "#### (i) t-SNE\n",
    "#### (ii) UMAP \n",
    "### (c)  Neural  \n",
    "#### (i) Autoencoders (AE)\n",
    "#### (ii) Contractive AE\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Representation Learning\n",
    "<!-- **Goal**: Learn meaningful features (often for downstream tasks). -->\n",
    "\n",
    "<!-- - Non-generatie -->\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Self-Supervised Learning (SSL)\n",
    "<!-- **Goal**: Use surrogate tasks to create labels from data itself. -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Evaluation\n",
    "<!-- - ðŸ”¹ Clustering metrics: NMI, ARI, purity\n",
    "- ðŸ”¹ Representation quality: linear probing, kNN accuracy\n",
    "- ðŸ”¹ Reconstruction error (for AE-type models) -->\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
